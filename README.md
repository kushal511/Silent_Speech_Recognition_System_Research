# ğŸ™ï¸ Silent Speech Recognition Preprocessing Pipeline

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![OpenCV](https://img.shields.io/badge/OpenCV-4.8+-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)
![Status](https://img.shields.io/badge/Status-Production%20Ready-success.svg)

**A robust, CPU-based preprocessing pipeline for Silent Speech Recognition using the Lip Reading in the Wild (LRW) dataset**

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Quick Start](#-quick-start) â€¢ [Documentation](#-documentation) â€¢ [Results](#-results)

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Usage](#-usage)
- [Pipeline Stages](#-pipeline-stages)
- [Configuration](#-configuration)
- [Results](#-results)
- [Project Structure](#-project-structure)
- [Contributing](#-contributing)
- [License](#-license)
- [Citation](#-citation)

---

## ğŸŒŸ Overview

Silent Speech Recognition (SSR) aims to recognize spoken words from visual information aloneâ€”specifically, lip movementsâ€”without audio. This project implements a **complete preprocessing pipeline** that transforms raw video clips into model-ready mouth region sequences for downstream deep learning tasks.

### ğŸ¯ Project Goals

- Extract mouth regions of interest (ROIs) from video frames
- Detect and track facial landmarks across temporal sequences
- Apply temporal smoothing to reduce jitter
- Generate consistent, high-quality training data for SSR models
- Provide comprehensive validation and quality control

---

## âœ¨ Features

### Core Capabilities

- ğŸ¥ **Robust Video Processing**: Handles various video formats (.mp4, .mpg) with error recovery
- ğŸ‘¤ **Accurate Face Detection**: MediaPipe Face Mesh or dlib for precise facial landmark detection
- ğŸ“ **Exact Landmark Extraction**: Targets actual lip boundaries (upper and lower lips separated)
- ğŸ¯ **ROI Computation**: Intelligent mouth region extraction based on exact lip boundaries
- ğŸ’¾ **Structured Output**: Organized data format ready for PyTorch/TensorFlow
- âš¡ **Multiprocessing**: Parallel processing for faster throughput
- ğŸ” **Quality Control**: Comprehensive validation and smoke testing
- ğŸ“Š **Detailed Logging**: Track processing statistics and failures

### Technical Highlights

- **CPU-Only**: No GPU required for preprocessing
- **Accurate Detection**: MediaPipe/dlib for precise lip boundary targeting
- **Production-Ready**: Tested on 1000+ videos with high success rate
- **Configurable**: YAML-based configuration for easy experimentation
- **Resumable**: Skip already processed videos automatically
- **Validated**: Comprehensive smoke tests and output verification

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Raw LRW Video (29 frames)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Frame Extractionâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Face Detection â”‚
                    â”‚ (MediaPipe/dlib)â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Landmark     â”‚
                    â”‚  Extraction    â”‚
                    â”‚ (Exact Boundaries)â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Lip Landmark  â”‚
                    â”‚   Selection    â”‚
                    â”‚ (Upper & Lower)â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Mouth ROI     â”‚
                    â”‚  Computation   â”‚
                    â”‚ (From Exact    â”‚
                    â”‚  Boundaries)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Crop & Resize â”‚
                    â”‚   (96Ã—96 RGB)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Preprocessed Output (Ready for Training)          â”‚
â”‚  â€¢ Mouth frames: 29 Ã— 96Ã—96 RGB images                        â”‚
â”‚  â€¢ Lip landmarks: 29 Ã— N Ã— 2 coordinates (exact boundaries)   â”‚
â”‚  â€¢ Metadata: Processing statistics & quality metrics          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager
- 2GB+ RAM
- 10GB+ disk space (for processed data)

### Step 1: Clone the Repository

```bash
git clone https://github.com/kushal511/Silent_Speech_Recognition_System.git
cd Silent_Speech_Recognition_System
```

### Step 2: Create Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Verify Installation

```bash
python3 verify_smoke_test_setup.py
```

Expected output:
```
âœ“ Python 3.x.x
âœ“ opencv-python
âœ“ numpy
âœ“ scipy
âœ“ PyYAML
âœ“ tqdm
âœ“ imageio
âœ“ SETUP COMPLETE
```

---

## âš¡ Quick Start

### 1. Test with Sample Data

Run the fast smoke test (30-60 seconds):

```bash
python3 run_fast_smoke_test.py test_lrw_dataset/data
```

### 2. Process Your Dataset

```bash
python3 run_preprocess.py \
    --input_dir lrw_dataset/data \
    --output_dir processed_lrw \
    --num_workers 4
```

### 3. Validate Outputs

```bash
python3 validate/run_validation.py \
    --data_dir processed_lrw \
    --output_dir validation_results
```

### 4. Load Preprocessed Data

```python
import numpy as np
from pathlib import Path

# Load mouth frames
frames_dir = Path("processed_lrw/WORD_CLASS/train/VIDEO_ID/frames")
frames = [np.array(Image.open(f)) for f in sorted(frames_dir.glob("*.png"))]

# Load landmarks
landmarks = np.load("processed_lrw/WORD_CLASS/train/VIDEO_ID/landmarks.npy")

print(f"Frames shape: {np.array(frames).shape}")  # (29, 96, 96, 3)
print(f"Landmarks shape: {landmarks.shape}")      # (29, 20, 2)
```

---

## ğŸ“– Usage

### Basic Preprocessing

Process entire dataset with default settings:

```bash
python3 run_preprocess.py \
    --input_dir lrw_dataset/data \
    --output_dir processed_lrw
```

### Process Specific Split

Process only training data:

```bash
python3 run_preprocess.py \
    --input_dir lrw_dataset/data \
    --output_dir processed_lrw \
    --split train
```

### Debug Mode

Enable visualizations and detailed logging:

```bash
python3 run_preprocess.py \
    --input_dir lrw_dataset/data \
    --output_dir processed_lrw \
    --debug
```

### Custom Configuration

Use custom config file:

```bash
python3 run_preprocess.py \
    --input_dir lrw_dataset/data \
    --output_dir processed_lrw \
    --config custom_config.yaml
```

### Parallel Processing

Utilize multiple CPU cores:

```bash
python3 run_preprocess.py \
    --input_dir lrw_dataset/data \
    --output_dir processed_lrw \
    --num_workers 8
```

---

## ğŸ”§ Pipeline Stages

### Stage 1: Video Loading
- Reads video files using OpenCV
- Extracts all frames as RGB arrays
- Validates frame count (expected: 29 frames)
- Handles corrupt/missing videos gracefully

### Stage 2: Face Detection & Landmark Extraction
- Uses MediaPipe Face Mesh (primary) or dlib (fallback)
- Detects faces and extracts precise facial landmarks
- Targets exact facial feature boundaries
- Provides high-quality landmark coordinates

### Stage 3: Lip Landmark Selection
- Extracts lip-specific landmarks from full face landmarks
- Separates upper and lower lip boundaries correctly
- MediaPipe: Uses specific indices for outer/inner lip contours
- dlib: Uses points 48-67 for complete lip region

### Stage 4: ROI Computation
- Calculates bounding box from exact lip boundary landmarks
- Adds 30% padding around mouth
- Enforces size constraints (64-128 pixels)
- Maintains square aspect ratio (1:1)

### Stage 5: Mouth Cropping
- Extracts mouth region from each frame
- Resizes to consistent 96Ã—96 pixels
- Preserves RGB color information
- Handles edge cases (partial faces)

### Stage 6: Output Saving
- Saves frames as PNG images
- Saves landmarks as NumPy arrays (.npy)
- Saves metadata as JSON
- Organizes by word class and split

---

## âš™ï¸ Configuration

The project includes two configuration files:

### config.yaml (Testing/Demo)
For testing and demos with sample GRID data (s1 directory):
```yaml
dataset:
  video_dir: "s1"  # Flat structure for test data
  video_extension: ".mpg"
```

Use with:
```bash
python3 demo_multiple_frames.py
python3 run_smoke_test.py lrw_dataset/data
```

### config_lrw.yaml (Production)
For processing the complete LRW dataset:
```yaml
dataset:
  video_dir: null  # Hierarchical structure (WORD_CLASS/SPLIT/)
  video_extension: ".mp4"
```

Use with:
```bash
python3 run_preprocess.py \
    --input_dir /path/to/lrw \
    --output_dir processed_lrw \
    --config config_lrw.yaml
```

### Key Configuration Options

```yaml
# Face Detection (MediaPipe/dlib for accurate lip boundaries)
face_detection:
  confidence_threshold: 0.5  # Minimum detection confidence
  model_selection: 0         # MediaPipe model (0 or 1)
  
# Mouth ROI
mouth_roi:
  padding_factor: 0.3        # 30% padding around lips
  target_size: [96, 96]      # Output dimensions
  min_size: 64               # Minimum ROI size
  max_size: 128              # Maximum ROI size
  
# Processing
processing:
  num_workers: 4             # Parallel workers
  skip_existing: true        # Resume capability
  max_videos: null           # Limit for testing
```

**Note**: Temporal smoothing has been removed from the pipeline as it's not required. Each frame is processed independently.

---

## ğŸ“Š Results

### Performance Metrics

| Metric | Value |
|--------|-------|
| **Face Detection** | MediaPipe/dlib accurate detection |
| **Landmark Accuracy** | Targets exact lip boundaries |
| **Processing Speed** | 2-5 seconds/video |
| **Success Rate** | High (tested on 1000+ videos) |
| **Output Quality** | 96Ã—96 RGB, no artifacts |
| **Memory Usage** | ~500 MB per worker |

### Output Format

```
processed_lrw/
â”œâ”€â”€ WORD_CLASS_1/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ VIDEO_001/
â”‚   â”‚   â”‚   â”œâ”€â”€ frames/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ frame_00.png  # 96Ã—96 RGB
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ frame_01.png
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ... (29 frames)
â”‚   â”‚   â”‚   â”œâ”€â”€ landmarks.npy      # (29, N, 2) - exact boundaries
â”‚   â”‚   â”‚   â””â”€â”€ metadata.json
â”‚   â”‚   â””â”€â”€ VIDEO_002/
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â””â”€â”€ WORD_CLASS_2/
    â””â”€â”€ ...
```

### Quality Assurance

- âœ… All frames validated for correct dimensions
- âœ… Landmarks target exact upper and lower lip boundaries
- âœ… Landmarks checked for NaN/infinity values
- âœ… ROI boxes verified within frame bounds
- âœ… Visual inspection via debug images

---

## ğŸ“ Project Structure

```
Silent_Speech_Recognition_System/
â”œâ”€â”€ ğŸ“„ README.md                    # This file
â”œâ”€â”€ ğŸ“„ LICENSE                      # MIT License
â”œâ”€â”€ âš™ï¸ config.yaml                  # Configuration
â”œâ”€â”€ ğŸ“‹ requirements.txt             # Dependencies
â”‚
â”œâ”€â”€ ğŸ Python Scripts
â”‚   â”œâ”€â”€ run_preprocess.py           # Main preprocessing pipeline
â”‚   â”œâ”€â”€ run_smoke_test.py           # Comprehensive testing
â”‚   â”œâ”€â”€ run_fast_smoke_test.py      # Quick validation
â”‚   â”œâ”€â”€ verify_smoke_test_setup.py  # Dependency checker
â”‚   â”œâ”€â”€ test_lrw_loading.py         # Dataset loading demo
â”‚   â””â”€â”€ example_usage.py            # Code examples
â”‚
â”œâ”€â”€ ğŸ“¦ src/                         # Core modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dataset.py                  # Dataset discovery
â”‚   â”œâ”€â”€ video_io.py                 # Video loading
â”‚   â”œâ”€â”€ face_landmarks.py           # Face detection
â”‚   â”œâ”€â”€ mouth_roi.py                # ROI extraction
â”‚   â”œâ”€â”€ smoothing.py                # Temporal smoothing
â”‚   â”œâ”€â”€ save_utils.py               # Output saving
â”‚   â”œâ”€â”€ visualize_debug.py          # Visualization
â”‚   â””â”€â”€ smoke_test_utils.py         # Testing utilities
â”‚
â””â”€â”€ ğŸ” validate/                    # Validation pipeline
    â”œâ”€â”€ run_validation.py           # Main validator
    â”œâ”€â”€ validate_shapes.py          # Shape checking
    â”œâ”€â”€ validate_detection.py       # Detection quality
    â”œâ”€â”€ validate_temporal.py        # Temporal consistency
    â”œâ”€â”€ validate_roi.py             # ROI quality
    â””â”€â”€ visualize_samples.py        # Visual QC
```

---


## ğŸ“œ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

```
MIT License

Copyright (c) 2024 Kushal Adhyaru

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
```

## ğŸ”— Resources

- **LRW Dataset**: [Oxford VGG](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html)
- **OpenCV Documentation**: [opencv.org](https://opencv.org/)
- **Python Documentation**: [python.org](https://www.python.org/)

---

---

## ğŸ™ Acknowledgments

- Oxford VGG for the LRW dataset
- OpenCV community for computer vision tools
- Python scientific computing community (NumPy, SciPy)

---

<div align="center">

**â­ Star this repository if you find it helpful!**

Made with â¤ï¸ for the Silent Speech Recognition community

</div>
